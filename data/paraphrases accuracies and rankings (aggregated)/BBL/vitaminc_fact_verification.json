{
  "paraphrases": {
    "default": {
      "template": "",
      "method": "default",
      "correct": true,
      "accs": {
        "t0_3b": 0.582351625,
        "t0pp": 0.695891559,
        "alpaca-7b": 0.135316074,
        "alpaca-13b": 0.135627016,
        "flan-t5-small": 0.409233921,
        "flan-t5-base": 0.647508621,
        "flan-t5-large": 0.362314334,
        "flan-t5-xl": 0.804766953,
        "flan-t5-xxl": 0.820278764
      },
      "ranks": {
        "t0_3b": 5,
        "t0pp": 3,
        "alpaca-7b": 9,
        "alpaca-13b": 8,
        "flan-t5-small": 6,
        "flan-t5-base": 4,
        "flan-t5-large": 7,
        "flan-t5-xl": 2,
        "flan-t5-xxl": 1
      }
    },
    "1": {
      "template": "You are now a very experienced judge. Based only on the information contained in a brief quote from Wikipedia, answer whether the related claim is True, False or Neither. Use Neither when the Wikipedia quote does not provide the necessary information to resolve the question.\\n\\n{context}\\nClaim{claim}\\nIs this True, False, or Neither?\n",
      "method": "manual",
      "correct": true,
      "accs": {
        "t0_3b": NaN,
        "t0pp": 0.689507544,
        "alpaca-7b": 0.132938251,
        "alpaca-13b": 0.135169744,
        "flan-t5-small": 0.369997073,
        "flan-t5-base": 0.649740279,
        "flan-t5-large": 0.363869174,
        "flan-t5-xl": 0.799004912,
        "flan-t5-xxl": 0.816784978
      },
      "ranks": {
        "t0_3b": 1,
        "t0pp": 4,
        "alpaca-7b": 9,
        "alpaca-13b": 8,
        "flan-t5-small": 6,
        "flan-t5-base": 5,
        "flan-t5-large": 7,
        "flan-t5-xl": 3,
        "flan-t5-xxl": 2
      }
    },
    "2": {
      "template": "Now you are a Vitaminc Fact Verifier. Based only on the information contained in a brief quote from Wikipedia, answer whether the related claim is True, False or Neither. Use Neither when the Wikipedia quote does not provide the necessary information to resolve the question.\\n\\n{context}\\nClaim{claim}\\nQuestionIs this True, False, or Neither?\\nYour answer:\n",
      "method": "manual",
      "correct": true,
      "accs": {
        "t0_3b": 0.558681488,
        "t0pp": 0.692891657,
        "alpaca-7b": 0.133889377,
        "alpaca-13b": 0.137712181,
        "flan-t5-small": 0.414282578,
        "flan-t5-base": 0.606223047,
        "flan-t5-large": 0.359369284,
        "flan-t5-xl": 0.79079169,
        "flan-t5-xxl": 0.817406893
      },
      "ranks": {
        "t0_3b": 5,
        "t0pp": 3,
        "alpaca-7b": 9,
        "alpaca-13b": 8,
        "flan-t5-small": 6,
        "flan-t5-base": 4,
        "flan-t5-large": 7,
        "flan-t5-xl": 2,
        "flan-t5-xxl": 1
      }
    },
    "3": {
      "template": "{context}\\nRead the above paragraph, and answer the following claim {claim}. Answer True, Flase, or Neither. Neither means the Wikipedia quote does not provide the necessary information to resolve the question. Answer:\n",
      "method": "manual",
      "correct": true,
      "accs": {
        "t0_3b": NaN,
        "t0pp": 0.635399163,
        "alpaca-7b": 0.230318993,
        "alpaca-13b": 0.143638432,
        "flan-t5-small": 0.476220101,
        "flan-t5-base": 0.508762002,
        "flan-t5-large": 0.374789639,
        "flan-t5-xl": 0.691080689,
        "flan-t5-xxl": 0.78201139
      },
      "ranks": {
        "t0_3b": 1,
        "t0pp": 4,
        "alpaca-7b": 8,
        "alpaca-13b": 9,
        "flan-t5-small": 6,
        "flan-t5-base": 5,
        "flan-t5-large": 7,
        "flan-t5-xl": 3,
        "flan-t5-xxl": 2
      }
    },
    "4": {
      "template": "Given a claim and its related information context from Wikipedia, determine whether the claim is True, False or Neither. Neither means the given information is not enough to decide if the claim is True or False, which is roughly equivalent to uncertain.\\n\\nContext:{context}\\nClaim{claim}\\nTrue, False or Neither?\n",
      "method": "manual",
      "correct": true,
      "accs": {
        "t0_3b": 0.567827642,
        "t0pp": 0.71112901,
        "alpaca-7b": 0.13288337,
        "alpaca-13b": 0.133212611,
        "flan-t5-small": 0.399356113,
        "flan-t5-base": 0.661739945,
        "flan-t5-large": 0.360887539,
        "flan-t5-xl": 0.805041313,
        "flan-t5-xxl": 0.81373018
      },
      "ranks": {
        "t0_3b": 5,
        "t0pp": 3,
        "alpaca-7b": 9,
        "alpaca-13b": 8,
        "flan-t5-small": 6,
        "flan-t5-base": 4,
        "flan-t5-large": 7,
        "flan-t5-xl": 2,
        "flan-t5-xxl": 1
      }
    },
    "5": {
      "template": "{context} Claim{claim}\\nBased on the context, is the claim true? false? Or Neither? Give your answer as one of \"True\", \"False\" or \"Neither\"\n",
      "method": "manual",
      "correct": true,
      "accs": {
        "t0_3b": NaN,
        "t0pp": 0.684093058,
        "alpaca-7b": 0.241458148,
        "alpaca-13b": 0.332656562,
        "flan-t5-small": 0.384411356,
        "flan-t5-base": 0.663550913,
        "flan-t5-large": 0.36833248,
        "flan-t5-xl": 0.81826663,
        "flan-t5-xxl": 0.801767051
      },
      "ranks": {
        "t0_3b": 1,
        "t0pp": 4,
        "alpaca-7b": 9,
        "alpaca-13b": 8,
        "flan-t5-small": 6,
        "flan-t5-base": 5,
        "flan-t5-large": 7,
        "flan-t5-xl": 2,
        "flan-t5-xxl": 3
      }
    },
    "6": {
      "template": "Based only on the information contained in the given context, please make a judgement whether the related claim is True, False or Neither.\\n\\n{context}\\n Claim{claim}\\nTrue, False, or Neither?\n",
      "method": "manual",
      "correct": true,
      "accs": {
        "t0_3b": 0.582900405,
        "t0pp": 0.692489207,
        "alpaca-7b": 0.144022539,
        "alpaca-13b": 0.309610039,
        "flan-t5-small": 0.387575913,
        "flan-t5-base": 0.661648512,
        "flan-t5-large": 0.36760079,
        "flan-t5-xl": 0.799919486,
        "flan-t5-xxl": 0.802242637
      },
      "ranks": {
        "t0_3b": 5,
        "t0pp": 3,
        "alpaca-7b": 9,
        "alpaca-13b": 8,
        "flan-t5-small": 6,
        "flan-t5-base": 4,
        "flan-t5-large": 7,
        "flan-t5-xl": 2,
        "flan-t5-xxl": 1
      }
    },
    "7": {
      "template": "Wikipedia{context}\\nSomeonebased on the given context, is the {claim} True, False, or Neither?\n",
      "method": "manual",
      "correct": true,
      "accs": {
        "t0_3b": NaN,
        "t0pp": 0.708202243,
        "alpaca-7b": 0.241458148,
        "alpaca-13b": 0.332656562,
        "flan-t5-small": 0.369082461,
        "flan-t5-base": 0.64862442,
        "flan-t5-large": 0.34436965,
        "flan-t5-xl": 0.798840284,
        "flan-t5-xxl": 0.802078009
      },
      "ranks": {
        "t0_3b": 1,
        "t0pp": 4,
        "alpaca-7b": 9,
        "alpaca-13b": 8,
        "flan-t5-small": 6,
        "flan-t5-base": 5,
        "flan-t5-large": 7,
        "flan-t5-xl": 3,
        "flan-t5-xxl": 2
      }
    },
    "8": {
      "template": "Evaluate the related claim as True, False, or Neither based solely on the information given in the short Wikipedia excerpt. Select Neither when the excerpt doesn't provide sufficient information to address the question.\\n{context}\\nClaim{claim}\\nAnswer(True, False, or Neither):\n",
      "method": "manual",
      "correct": true,
      "accs": {
        "t0_3b": 0.577741981,
        "t0pp": 0.70522058,
        "alpaca-7b": 0.132791921,
        "alpaca-13b": 0.132791921,
        "flan-t5-small": 0.372448233,
        "flan-t5-base": 0.653087735,
        "flan-t5-large": 0.364381357,
        "flan-t5-xl": 0.800175607,
        "flan-t5-xxl": 0.80961442
      },
      "ranks": {
        "t0_3b": 5,
        "t0pp": 3,
        "alpaca-7b": 8,
        "alpaca-13b": 8,
        "flan-t5-small": 6,
        "flan-t5-base": 4,
        "flan-t5-large": 7,
        "flan-t5-xl": 2,
        "flan-t5-xxl": 1
      }
    },
    "9": {
      "template": "Input{claim}\\nVerify the factually of the claim based on the following context\\n{context}\\n\\n- \"True\" if the claim is factually correct\\n- \"False\" if the claim is factually incorrect\\n- \"Neither\" if the factuality cannot be determined. Output you answer with one of \"True\", \"False\", or \"Neither\\\". Answer:\n",
      "method": "manual",
      "correct": true,
      "accs": {
        "t0_3b": NaN,
        "t0pp": 0.65343529,
        "alpaca-7b": 0.23520267,
        "alpaca-13b": 0.579272747,
        "flan-t5-small": 0.496872028,
        "flan-t5-base": 0.600479245,
        "flan-t5-large": 0.371240945,
        "flan-t5-xl": 0.751207292,
        "flan-t5-xxl": 0.766993463
      },
      "ranks": {
        "t0_3b": 1,
        "t0pp": 4,
        "alpaca-7b": 9,
        "alpaca-13b": 6,
        "flan-t5-small": 7,
        "flan-t5-base": 5,
        "flan-t5-large": 8,
        "flan-t5-xl": 3,
        "flan-t5-xxl": 2
      }
    },
    "10": {
      "template": "Context{context}\\nNow classify this claim into one of 'True', 'False', or 'Neither'.\\n{claim}",
      "method": "manual",
      "correct": true,
      "accs": {
        "t0_3b": 0.593528211,
        "t0pp": 0.669349551,
        "alpaca-7b": 0.234416157,
        "alpaca-13b": 0.228508189,
        "flan-t5-small": 0.412160679,
        "flan-t5-base": 0.63022244,
        "flan-t5-large": 0.368936124,
        "flan-t5-xl": 0.792181909,
        "flan-t5-xxl": 0.7914868
      },
      "ranks": {
        "t0_3b": 5,
        "t0pp": 3,
        "alpaca-7b": 8,
        "alpaca-13b": 9,
        "flan-t5-small": 6,
        "flan-t5-base": 4,
        "flan-t5-large": 7,
        "flan-t5-xl": 1,
        "flan-t5-xxl": 2
      }
    }
  }
}
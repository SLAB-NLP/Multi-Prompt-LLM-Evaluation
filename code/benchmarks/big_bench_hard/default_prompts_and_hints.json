{
  "causal_judgement": {
    "input_templates": [
      "Q: How would a typical person answer each of the following questions about causation?\n{question}\nOptions:\n- Yes\n- No\nA:"
    ],
    "hints": "Task name: causal judgement\nFormat: classification (yes/no)\nDescription: Answer questions about causal attribution.\nKeywords: question\n\nExample:\nQ: How would a typical person answer each of the following questions about causation?\nFrank T., had an ongoing dispute with his neighbor over a stretch of land and one day decided to shoot his neighbor in the body. Frank T. had no experience with guns, his hand slipped on the barrel of the gun, and the shot went wild. Nonetheless, the bullet bounced off a large boulder several feet away and hit the neighbor's body, causing significant injury. Did Frank T. intentionally shoot his neighbor in the body?\nOptions:\n- Yes\n- No\nA: No"
  },
  "disambiguation_qa": {
    "input_templates": [
      "Q: In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\nSentence: {sentence}\nOptions:\n{options}\nA:"
    ],
    "hints": "Task name: disambiguation qa\nFormat: multi_choice\nDescription: Clarify the meaning of sentences with ambiguous pronouns.\nKeywords: sentence, options\n\nExample:\nQ: In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\nSentence: The chief told the counselor that they took the day off.\nOptions:\n(A) The chief took the day off\n(B) The counselor took the day off\n(C) Ambiguous\nA: (A)"
  },
  "dyck_languages": {
    "input_templates": [
      "Q: Complete the rest of the sequence, making sure that the parentheses are closed properly.\nInput: {input}\nA:"
    ],
    "hints": "Task name: dyck languages\nFormat: seq2seq\nDescription: Correctly close a Dyck-n word.\nKeywords: input\n\nExample:\ninput: [ { [\nA: ] } ]"
  },
  "formal_fallacies": {
    "input_templates": [
      "Q: \"{input}\"\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\nOptions:\n- valid\n- invalid\nA:"
    ],
    "hints": "Task name: formal fallacies\nFormat: classification (valid/invalid)\nDescription: Distinguish deductively valid arguments from formal fallacies.\nKeywords: input\n\nExample:\nQ: \"It is not always easy to see who is related to whom -- and in which ways. The following argument pertains to this question: To begin with, Lesley is a close friend of Fernando. Moreover, being a close friend of Fernando or a schoolmate of Lowell is sufficient for being a great-grandfather of Leroy. It follows that Lesley is a great-grandfather of Leroy.\"\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\nOptions:\n- valid\n- invalid\nA: valid"
  },
  "geometric_shapes": {
    "input_templates": [
      "Q: This SVG path element {svg_path_element} draws a\nOptions:\n{options}\nA:"
    ],
    "hints": "Task name: geometric shapes\nFormat: multi_choice\nDescription: Name geometric shapes from their SVG paths.\nKeywords: svg_path_element, options\n\nExample:\nQ: This SVG path element <path d=\"M 31.00,73.00 L 32.00,59.00 L 44.00,50.00 L 49.00,41.00 L 64.00,37.00 L 71.00,55.00 L 64.00,76.00 L 52.00,61.00 L 31.00,73.00\"/> draws a\nOptions:\n(A) circle\n(B) heptagon\n(C) hexagon\n(D) kite\n(E) line\n(F) octagon\n(G) pentagon\n(H) rectangle\n(I) sector\n(J) triangle\nA: (F)"
  },
  "hyperbaton": {
    "input_templates": [
      "Q: Which sentence has the correct adjective order:\nOptions:\n{options}\nA:"
    ],
    "hints": "Task name: hyperbaton\nFormat: multi_choice\nDescription: Order adjectives correctly in English sentences.\nKeywords: options\n\nExample:\nQ: Which sentence has the correct adjective order:\nOptions:\n(A) rubber terrible ship\n(B) terrible rubber ship\nA: (B)"
  },
  "logical_deduction_three_objects": {
    "input_templates": [
      "Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n{paragraph}\nOptions:\n{options}\nA:"
    ],
    "hints": "Task name: logical deduction three objects\nFormat: multi_choice\nDescription: A logical deduction task which requires deducing the order of a sequence of objects.\nKeywords: paragraph, options\n\nExample:\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph.\nIn a golf tournament, there were three golfers: Amy, Eli, and Eve. Eve finished above Amy. Eli finished below Amy.\nOptions:\n(A) Amy finished last\n(B) Eli finished last\n(C) Eve finished last\nA: (B)"
  },
  "logical_deduction_five_objects": {
    "input_templates": [
      "Q: The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n{paragraph}\nOptions:\n{options}\nA:"
    ],
    "hints": "Task name: logical deduction five objects\nFormat: multi_choice\nDescription: A logical deduction task which requires deducing the order of a sequence of objects.\nKeywords: paragraph, options\n\nExample:\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph.\nIn a golf tournament, there were three golfers: Amy, Eli, and Eve. Eve finished above Amy. Eli finished below Amy.\nOptions:\n(A) Amy finished last\n(B) Eli finished last\n(C) Eve finished last\nA: (B)"
  },
  "logical_deduction_seven_objects": {
    "input_templates": [
      "Q: The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph.\n{paragraph}\nOptions:\n{options}\nA:"
    ],
    "hints": "Task name: logical deduction seven objects\nFormat: multi_choice\nDescription: A logical deduction task which requires deducing the order of a sequence of objects.\nKeywords: paragraph, options\n\nExample:\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph.\nIn a golf tournament, there were three golfers: Amy, Eli, and Eve. Eve finished above Amy. Eli finished below Amy.\nOptions:\n(A) Amy finished last\n(B) Eli finished last\n(C) Eve finished last\nA: (B)"
  },
  "movie_recommendation": {
    "input_templates": [
      "Q: Find a movie similar to {movie_list}:\nOptions:\n{options}\nA:"
    ],
    "hints": "Task name: movie recommendation\nFormat: multi_choice\nDescription: Recommend movies similar to the given list of movies.\nKeywords: movie_list, options\n\nExample:\nQ: Find a movie similar to Star Wars Episode IV - A New Hope, Indiana Jones and the Last Crusade, Star Wars Episode V - The Empire Strikes Back, The Big Lebowski:\nOptions:\n(A) Tetsuo\n(B) the Ironman\n(C) The Princess Bride\n(D) The Barkley Marathons The Race That Eats Its Young\n(E) Bug\nA: (C)"
  },
  "navigate": {
    "input_templates": [
      "Q: If you follow these instructions, do you return to the starting point?\n{instructions}\nOptions:\n- Yes\n- No\nA:"
    ],
    "hints": "Task name: navigate\nFormat: classification (yes/no)\nDescription: Given a series of navigation instructions, determine whether one would end up back at the starting point.\nKeywords: instructions\n\nExample:\nQ: If you follow these instructions, do you return to the starting point? Turn left. Turn around. Turn left. Take 7 steps. Take 2 steps. Take 4 steps. Take 8 steps.\nOptions:\n- Yes\n- No\nA: No"
  },
  "object_counting": {
    "input_templates": [
      "Q: I have {object_list}. How many {object_type} do I have?\nA:"
    ],
    "hints": "Task name: object counting\nFormat: seq2seq\nDescription: Questions that involve enumerating objects and asking the model to count them.\nKeywords: object_list, object_type\n\nExample:\nQ: I have a blackberry, a clarinet, a nectarine, a plum, a strawberry, a banana, a flute, an orange, and a violin. How many fruits do I have?\nA: 6"
  },
  "penguins_in_a_table": {
    "input_templates": [
      "Q: Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg)\n{table_description}\n{question}\nOptions:\n{options}\nA:"
    ],
    "hints": "Task name: penguins in a table\nFormat: multi_choice\nDescription: Answer questions about a table of penguins and their attributes.\nKeywords: table_description, question, options\n\nExample:\nQ: Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg)\nLouis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  We now add a penguin to the table: James, 12, 90, 12\nHow many penguins are less than 8 years old?\nOptions:\n(A) 1\n(B) 2\n(C) 3\n(D) 4\n(E) 5\nA: (B)"
  },
  "ruin_names": {
    "input_templates": [
      "Q: Which of the following is a humorous edit of this artist or movie name: '{artist_or_movie_name}'?\nOptions:\n{options}\nA:"
    ],
    "hints": "Task name: ruin names\nFormat: multi_choice\nDescription: Select the humorous edit that 'ruins' the input movie or musical artist name.\nKeywords: artist_or_movie_name, options\n\nExample:\nQ: Which of the following is a humorous edit of this artist or movie name: 'whitesnake'?\nOptions:\n(A) whitesnape\n(B) whitesnapke\n(C) whitesnuake\n(D) mwhitesnake\nA: (A)"
  },
  "salient_translation_error_detection": {
    "input_templates": [
      "Q: The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.\nSource: {source}\nTranslation: {translation}\nThe translation contains an error pertaining to\nOptions:\n{options}\nA:"
    ],
    "hints": "Task name: salient translation error detection\nFormat: multi_choice\nDescription: Detect the type of error in an English translation of a German source sentence.\nKeywords: source, translation, options\n\nExample:\nQ: The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: In der Liste der Baudenkmale in Lenzen (Elbe) sind alle Baudenkmale der brandenburgischen Stadt Lenzen (Elbe) und ihrer Ortsteile aufgelistet.\nTranslation: In the list of architectural monuments in Lenzen all architectural monuments of the Brandenburg city of Lenzen and its districts are listed.\nThe translation contains an error pertaining to\nOptions:\n(A) Modifiers or Adjectives\n(B) Numerical Values\n(C) Negation or Antonyms\n(D) Named Entities\n(E) Dropped Content\n(F) Facts\nA: (D)"
  },
  "snarks": {
    "input_templates": [
      "Q: Which statement is sarcastic?\nOptions:\n{options}\nA:"
    ],
    "hints": "Task name: snarks\nFormat: multi_choice\nDescription: Determine which of two sentences is sarcastic.\nKeywords: options\n\nExample:\nQ: Which statement is sarcastic?\nOptions:\n(A) Yes, because having interests and actively researching them is a huge waste\n(B) Yes, because having interests and actively researching them is a huge deal\nA: (A)"
  },
  "sports_understanding": {
    "input_templates": [
      "Q: Is the following sentence plausible?\n\"{sentence}\"\nA:"
    ],
    "hints": "Task name: sports understanding\nFormat: classification (yes/no)\nDescription: Determine whether an artificially constructed sentence relating to sports is plausible or not.\nKeywords: sentence\n\nExample:\nQ: Is the following sentence plausible? \"Bam Adebayo scored a reverse layup in the Western Conference Finals.\"\nA: yes"
  },
  "web_of_lies": {
    "input_templates": [
      "Q: {story}. Does {person_name} tell the truth?\nA:"
    ],
    "hints": "Task name: web of lies\nFormat: classification (yes/no)\nDescription: Evaluate a random boolean function expressed as a word problem.\nKeywords: story, person_name\n\nExample:\nQ: Question: Fidel tells the truth. Jerry says Fidel tells the truth. Vina says Jerry tells the truth. Millicent says Vina lies. Raymond says Millicent lies. Does Raymond tell the truth?\nA: Yes"
  },
  "word_sorting": {
    "input_templates": [
      "Q: Sort the following words alphabetically: List: {word_list}\nA:"
    ],
    "hints": "Task name: word sorting\nFormat: seq2seq\nDescription: Sort a list of words.\nKeywords: word_list\n\nExample:\nQ: Sort the following words alphabetically: List: oven costume counterpart\nA: costume counterpart oven"
  }
}